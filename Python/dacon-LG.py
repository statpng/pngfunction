# -*- coding: utf-8 -*-
"""Dacon-LG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mfYwzLMxZLyN3ZGrN-kQxCPOc8_BfcaS
"""

# from google.colab import files
# uploaded = files.upload("~/")

from google.colab import drive

drive.mount('/content/gdrive', force_remount=True)

!ls "gdrive/MyDrive/data/dacon/LG"

from IPython.display import display
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from tqdm import tqdm
import lightgbm as lgb
from sklearn.metrics import *
from sklearn.model_selection import KFold
import warnings
import missingno as msno
from sklearn.preprocessing import LabelEncoder

import datetime as dt
warnings.filterwarnings(action='ignore')

# 필요한 함수 정의
def make_datetime(x):
    # string 타입의 Time column을 datetime 타입으로 변경
    x     = str(x)
    year  = int(x[:4])
    month = int(x[4:6])
    day   = int(x[6:8])
    hour  = int(x[8:10])
    #mim  = int(x[10:12])
    #sec  = int(x[12:])
    return dt.datetime(year, month, day, hour)

def string2num(x):
    # (,)( )과 같은 불필요한 데이터 정제
    x = re.sub(r"[^0-9]+", '', str(x))
    if x =='':
        return 0
    else:
        return int(x)


# from xgboost import XGBClassifier
# from sklearn.model_selection import train_test_split
# from sklearn.model_selection import KFold
# from numba import jit
# import time
# import gc
# import matplotlib.pyplot as plt

"""## User-defined functions"""

def get_df_name(df):
    name =[x for x in globals() if globals()[x] is df][0]
    return name

"""## Data import"""

# Check your data path given
# PATH = "gdrive/My Drive/data/dacon/LG/
PATH = "./data/"

sample_submission = pd.read_csv(PATH+"/sample_submission.csv")

train_err = pd.read_csv(PATH+'/train_err_data.csv')
train_prob = pd.read_csv(PATH+'/train_problem_data.csv')
train_qual = pd.read_csv(PATH+'/train_quality_data.csv')

test_err = pd.read_csv(PATH+'/test_err_data.csv')
test_qual = pd.read_csv(PATH+'/test_quality_data.csv')


"""## Generate the response data (Y)"""
problem = np.zeros(15000)
problem[train_prob.user_id.unique()-10000] = 1
problem.shape


"""## Check the datasets"""

for x in [train_err, train_qual, train_prob, test_err, test_qual]:
  print( "<<", get_df_name(x), ">>" )
  display( x.shape )

for x in [train_err, train_qual, train_prob, test_err, test_qual]:
  print( "<<", get_df_name(x), ">>" )
  display( x.head() )

"""## Check the types of data"""

for x in [train_err, train_qual, train_prob, test_err, test_qual]:
  print( "<<", get_df_name(x), ">>" )
  display( x.info() )

"""## EDA"""

train_err["model_nm"].value_counts()

for x in [train_err, train_qual]:
  print( "<<", get_df_name(x), ">>" )
  display( x.describe() )

"""## Check whether there exist any missing values

### Check the null values by column_wise
"""

for x in [train_err, train_qual]:
  print( "<<", get_df_name(x), ">>" )
  display( x.isnull().sum() )

"""### Check the null values by row_wise"""

train_qual.isnull().sum(axis=1).sort_values(ascending=False)

"""## Get a matrix of the error distribution"""

# Missing Values
msno.matrix(train_qual)

train_qual["quality_0"].hist()

train_qual["quality_0"][ train_qual["quality_0"] < 150000 ].describe()

train_qual["quality_0"].value_counts()

"""## Distribution for each features in train_err"""

display( "train_err[user_id]", train_err["user_id"].agg(["min", "max"]) )
display( "test_err[user_id]", test_err["user_id"].agg(["min", "max"]) )

display( "train_err[time]", train_err["time"].agg(["min", "max"]) )
display( "test_err[time]", test_err["time"].agg(["min", "max"]) )

display( "train_err[errtype]", train_err["errtype"].agg(["min", "max"]) )
display( "test_err[errtype]", test_err["errtype"].agg(["min", "max"]) )


"""
# train_qual

### Training
"""

for j in ["quality_5", "quality_7", "quality_8", "quality_9", "quality_10"]:
    display( j, train_qual[j].str.contains(",").sum() )

for j in ["quality_5", "quality_7", "quality_8", "quality_9", "quality_10"]:
    train_qual[j] = train_qual[j].map(lambda x: str(x).split(",")[0])

    display( j, (~train_qual[j].map(lambda x: len( re.findall("[-]{0,1}[0-9]", str(x)) ) > 0 ) ).sum() )


train_qual["quality_5"][ ~train_qual["quality_5"].map(lambda x: len( re.findall("[-]{0,1}[0-9]", str(x)) ) > 0 ) ]

"""### Test"""

quality_comma = []

for j in ["quality_"+str(j) for j in range(0, 13)]:
    if( test_qual[j].map(lambda x: len( re.findall(",", str(x)) ) > 0 ).sum() ) :
        display( j, test_qual[j].str.contains(",").sum() )
        quality_comma.append( j )


for j in quality_comma:
    test_qual[j] = test_qual[j].map(lambda x: str(x).split(",")[0])
    display( j, (~test_qual[j].map(lambda x: len( re.findall("[-]{0,1}[0-9]", str(x)) ) > 0 ) ).sum() )


for j in quality_comma:
    print( test_qual[j][ ~test_qual[j].map(lambda x: len( re.findall("[-]{0,1}[0-9]", str(x)) ) > 0 ) ] )

train_qual[["quality_0", "quality_1", "quality_2", "quality_3",
            "quality_4", "quality_5", "quality_6", "quality_7",
            "quality_8", "quality_9", "quality_10", "quality_11",
            "quality_12"]] =  train_qual[["quality_0", "quality_1", "quality_2", "quality_3",
                        "quality_4", "quality_5", "quality_6", "quality_7",
                        "quality_8", "quality_9", "quality_10", "quality_11",
                        "quality_12"]].apply(pd.to_numeric)

train_qual[["quality_5", 
            "quality_7",
            "quality_8", 
            "quality_9", 
            "quality_10"]]



"""## Correlation between quality scores"""

eda_qual_corr = train_qual.filter(regex="quality_[0-9]+").corr(method = "spearman")
sns.heatmap(eda_qual_corr, cmap="coolwarm", square=True, center=0)


"""## train_err

### fwver
"""

train_err_errtype_set = set(train_err['fwver'])
test_err_errtype_set = set(test_err['fwver'])

display( "The number of categories that uniquely exists in the train :", len( train_err_errtype_set ) )
display( "The number of categories that uniquely exists in the test :", len( test_err_errtype_set ) )

display( "Existing in train but not in test", train_err_errtype_set.difference(test_err_errtype_set) )
display( "Existing in test but not in train", test_err_errtype_set.difference(train_err_errtype_set) )

train_err["fwver"].value_counts()














""" # 2.Generating New Variables """

""" ## Preprocess"""
def preprocess_err(df):
    ########## Date #########
    import datetime
    time = train_err['time']
    train_err['time2'] = pd.to_datetime(time, format='%Y%m%d%H%M%S')
    train_err['date'] = train_err['time2'].dt.date
    train_err['month'] = train_err['time2'].dt.month
    train_err['day'] = train_err['time2'].dt.day
    train_err['hour'] = train_err['time2'].dt.hour


    ######### errcode #########
    err = df["errcode"]

    df.drop(df.index[err.isnull()], inplace=True)

    # New Variable (errcode_numeric) :: whether each variable is numeric or not
    df["errcode_numeric"] = err.map(lambda x: 1 if str(x).replace("-", "").isdigit() else 0)

    # New Variable (errcode_digit) :: the number of digits for each
    df["errcode_digits"] = err.map(lambda s: len(re.findall("[0-9]", str(s))) if str(s).isdigit() else 0)

    # errcode_sparse
    idx_errcode_100000 = list(err.value_counts()[err.value_counts() > 100000].index)
    df["errcode_sparse"] = err
    df.loc[~err.isin(idx_errcode_100000), "errcode_sparse"] = "sparse"

    # df["WS"] = (df["WS"] * 10).astype("int32")


    ######## FWVER ########

    ### fwver_1, fwver_2, fwver_3

    # fwver = train_err["fwver"]

    # train_err["fwver_1"] = 0
    # train_err["fwver_1"] = fwver.str.split(".").map(lambda x: x[0]).astype(int)
    # train_err["fwver_2"] = 0
    # train_err["fwver_2"] = fwver.str.split(".").map(lambda x: x[1]).astype(int)
    # train_err["fwver_3"] = 0
    # train_err["fwver_3"] = fwver.str.split(".").map(lambda x: x[2]).astype(int)

    # train_err["fwver"] = train_err["fwver"].head().str.replace(".", "")
    # train_err["fwver"] = pd.to_numeric( train_err["fwver"] )


    return df


train_err2 = preprocess_err(train_err)
train_err2

"""### FEATURE TYPE
"""
train_err.head(2)
train_qual.head(2)

train_err.columns
train_qual.columns


FEATURES = ["date", "month", "day", "hour"]
FEATURES_CONTINUOUS = ["date", "month", "day", "hour",
                       "quality_0", "quality_1", "quality_2", "quality_3", "quality_4", "quality_5",
                       "quality_6", "quality_7", "quality_8", "quality_9", "quality_10",
                       "quality_11", "quality_12"]
FEATURES_CATEGORICAL = ["model_nm", "fwver", "errtype",
                        "errcode", "errcode_numeric", "errcode_digits", "errcode_sparse"]
TARGET = "problem"




""" ## Label Encoder """

def load_train_df(path):
    df = pd.read_csv(path)
    df = preprocess(df)
    #     df = range_mapping(df)
    label_encoder_dict = {}
    for col in FEATURES_CATEGORICAL:
        le = LabelEncoder()
        le = le.fit(df[col])
        df[col] = le.transform(df[col])
        label_encoder_dict[col] = le

    return df, label_encoder_dict

# x3: errcode_numeric
tmp = train_err[["user_id", "errcode_sparse"]].drop_duplicates()["errcode_sparse"]

# 라벨 인코더 생성
encoder = LabelEncoder()

# X_train데이터를 이용 피팅하고 라벨숫자로 변환한다
encoder.fit(tmp)
tmp_encoded = encoder.transform(tmp)

# X_test데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가한다 (중요!!!)
for label in np.unique(tmp_new):
    if label not in encoder.classes_: # unseen label 데이터인 경우( )
        encoder.classes_ = np.append(encoder.classes_, label) # 미처리 시 ValueError발생
tmp_new_encoded = encoder.transform(tmp_new)
















"""## errtype
- I found a missing value for error code in train_err. 
  
  So, I'm gonna remove the row containing the missing value.
"""

display( train_err["errcode"].isnull().sum() )
display( train_err[ train_err["errcode"].isnull() ] )
display( train_err["errcode"].isnull().sum() )


"""### errcode

- There are a lot of categories in errcode of train_err.

  However, it consumes many degrees of freedom in our model.

  So, I will divide them into some categories by my intuition.

  (1) The more number of categories is, the more it is important.
  That is to say, 1, 0, connection timeout, ... could be an important feature.

  (2) There are lots of numeric values (13220082) compared to non-numeric values (3334581).

1. 0 vs 1 vs others
2. Connection timeout vs others
3. Numeric vs Character
4.
"""

train_err["errcode"].value_counts()[train_err["errcode"].value_counts() > 10000]

train_err["errcode"].value_counts()

train_err["errcode"].str.match("[0-9]+").value_counts()








"""#### New Variable (errcode_numeric) :: whether each variable is numeric or not"""
pd.crosstab( train_err["errcode"], train_err["errcode_numeric"], margins=True )

"""#### New Variable (errcode_digit) :: the number of digits for each """
pd.crosstab( train_err["errcode"], train_err["errcode_digits"], margins=True)

"""## errcode_sparse """
train_err["errcode_sparse"].value_counts()




"""## META INFORMATION
"""

"""### Input data dimension"""
# 데이터 설명을 확인하면
# ueser_id가 10000부터 24999까지 총 15000개가 연속적으로 존재.
train_user_id_max = 24999
train_user_id_min = 10000
train_user_number = 15000




"""## Convert to input data"""

""" #### New Input Data """

new_freq = train_err["user_id"].value_counts().reset_index().sort_values(["index"]).rename(columns = {"user_id": "freq"})["freq"]

""" #### model_nm """
# x1: model_nm
N_model_nm = train_err["model_nm"].value_counts().shape[0]
x1 = np.zeros((train_user_number, N_model_nm))
tmp = train_err[["user_id", "model_nm"]].drop_duplicates()
tmp["model_nm_numeric"] = pd.to_numeric( tmp["model_nm"].replace({"model_0": 1,
                                                                    "model_1": 2,
                                                                    "model_2": 3,
                                                                    "model_3": 4,
                                                                    "model_4": 5,
                                                                    "model_5": 6,
                                                                    "model_6": 7,
                                                                    "model_7": 8,
                                                                    "model_8": 9}) )

for person_idx, idx_model_nm in tqdm(tmp[["user_id", "model_nm_numeric"]].values):
    x1[person_idx - train_user_id_min, idx_model_nm - 1] += 1

# x2: errcode_sparse
tmp = train_err[["user_id", "errcode_sparse"]].drop_duplicates()
tmp["errcode_sparse_numeric"] = pd.to_numeric( tmp["errcode_sparse"].replace({"1": 1,
                                                                            "0": 2,
                                                                            "connection timeout": 3,
                                                                            "B-A8002": 4,
                                                                            "80": 5,
                                                                            "79": 6,
                                                                            "14": 7,
                                                                            "active": 8,
                                                                            "2": 9,
                                                                            "84": 10,
                                                                            "85": 11,
                                                                            "standby": 12,
                                                                            "NFANDROID2": 13,
                                                                            "connection fail to establish": 14,
                                                                            "sparse": 15 }) )

N_errcode_sparse = train_err["errcode_sparse"].value_counts().shape[0]
x2 = np.zeros((train_user_number, N_errcode_sparse))
for person_idx, idx in tqdm(tmp[["user_id", "errcode_sparse_numeric"]].values):
    x2[person_idx - train_user_id_min, idx - 1] += 1




# x3: errcode_numeric
tmp = train_err[["user_id", "errcode_sparse"]].drop_duplicates()["errcode_sparse"]

# 라벨 인코더 생성
encoder = LabelEncoder()

# X_train데이터를 이용 피팅하고 라벨숫자로 변환한다
encoder.fit(tmp)
tmp_encoded = encoder.transform(tmp)

# X_test데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가한다 (중요!!!)
for label in np.unique(tmp_new):
    if label not in encoder.classes_: # unseen label 데이터인 경우( )
        encoder.classes_ = np.append(encoder.classes_, label) # 미처리 시 ValueError발생
tmp_new_encoded = encoder.transform(tmp_new)








tmp["errcode_sparse_numeric"] = pd.to_numeric( tmp["errcode_sparse"].replace({"1": 1,
                                                                            "0": 2,
                                                                            "connection timeout": 3,
                                                                            "B-A8002": 4,
                                                                            "80": 5,
                                                                            "79": 6,
                                                                            "14": 7,
                                                                            "active": 8,
                                                                            "2": 9,
                                                                            "84": 10,
                                                                            "85": 11,
                                                                            "standby": 12,
                                                                            "NFANDROID2": 13,
                                                                            "connection fail to establish": 14,
                                                                            "sparse": 15 }) )

N_errcode_sparse = train_err["errcode_sparse"].value_counts().shape[0]
x2 = np.zeros((train_user_number, N_errcode_sparse))
for person_idx, idx in tqdm(tmp[["user_id", "errcode_sparse_numeric"]].values):
    x2[person_idx - train_user_id_min, idx - 1] += 1




train_err.head(2)


"""### errtype"""
# user_id와 errtype만을 사용하여 데이터 셋 생성
# 모든 일자에 대해서 errtype별 발생 건수를 count
# pandas의 groupby를 활용할 경우 큰 연산 자원이 필요.
# numpy로 placeholder를 만들어 구현함.
id_error = train_err[['user_id','errtype']].values
error = np.zeros((train_user_number,42))

for person_idx, err in tqdm(id_error):
    # person_idx - train_user_id_min 위치에 person_idx, errtype에 해당하는 error값을 +1
    error[person_idx - train_user_id_min,err - 1] += 1
error.shape

# model_nm
train_err["model_nm"].value_counts()

train_err.groupby(["user_id"]).agg(['mean', 'count']).head()

def npmean(x): return np.mean(x)
def npstd(x): return np.std(x)

user_id_unique = train_err['user_id'].unique()

numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
num_numeric = train_err.drop(["user_id"], axis=1).select_dtypes(include = numeric_types).shape[1]


error2 = np.zeros((train_user_number, num_numeric * 5))
for person_idx in tqdm(user_id_unique):
    tmp = train_err[ train_err["user_id"] == person_idx ]
    error2[person_idx - train_user_id_min,] = tmp.groupby("user_id").agg([npmean, npstd, "count", "min", "max"])


res_aggr = train_err.groupby(["user_id"]).agg([npmean, 'count', "min", "max", npstd])
np.array( )
error2 = pd.concat([ pd.DataFrame(error), pd.DataFrame( ) ], axis=1)
error2.head()

train_err["errtype"].value_counts()



#

train_prob_err = pd.merge( train_prob, train_err, on=["user_id"], how="left" )
train_prob_err
train_prob

train_err.head(2)
train_prob.head(2)

#




# model_nm

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
needenco = ['age_group', 'gender', 'race', 'religion']
for i in needenco:
  x_train[i] = encoder.fit_transform(x_train[i])
  test[i] = encoder.transform(test[i])

"""# 3. Light-gbm 모델 훈련

## Splitting the observations through cross-validation
"""

k_fold = KFold(n_splits = 5, shuffle = True, random_state = 2021-01-12)

Modelling

# clf1 = RandomForestClassifier(n_estimators=500)
# clf2 = LGBMClassifier()
# clf3 = GradientBoostingClassifier()
# soft_vote  = VotingClassifier([('r1',clf1), ('r2', clf2), ('r3',clf3)], voting='soft')
# soft_vote.fit(x_train, y_train)

"""Submission"""

model = soft_vote
pred_y = model.predict_proba(test)
pred_y = pred_y[:,1]

submission = pd.DataFrame({
    "index" : index,
    "voted" : pred_y
})
submission.to_csv('../data/model1.csv', index=False)

# Commented out IPython magic to ensure Python compatibility.
# Example

# Process data
id_test = test_df['id'].values
id_train = train_df['id'].values
y = train_df['target']

start = time.time()
for n_c, (f1, f2) in enumerate(combs):
    name1 = f1 + "_plus_" + f2
    print('current feature %60s %4d in %5.1f'
#           % (name1, n_c + 1, (time.time() - start) / 60), end='')
    print('\r' * 75, end='')
    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + "_" + train_df[f2].apply(lambda x: str(x))
    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + "_" + test_df[f2].apply(lambda x: str(x))
    # Label Encode
    lbl = LabelEncoder()
    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))
    train_df[name1] = lbl.transform(list(train_df[name1].values))
    test_df[name1] = lbl.transform(list(test_df[name1].values))

    train_features.append(name1)
    
X = train_df[train_features]
test_df = test_df[train_features]

f_cats = [f for f in X.columns if "_cat" in f]

# 변수 이름 변경
# error  -> train_x
# problem-> train_y

train_x = np.array(error2)
train_y = problem
del error, problem
print(train_x.shape)
print(train_y.shape)

# Train
#-------------------------------------------------------------------------------------
# validation auc score를 확인하기 위해 정의
def f_pr_auc(probas_pred, y_true):
    labels=y_true.get_label()
    p, r, _ = precision_recall_curve(labels, probas_pred)
    score=auc(r,p) 
    return "pr_auc", score, True
#-------------------------------------------------------------------------------------
models     = []
recalls    = []
precisions = []
auc_scores   = []
threshold = 0.5
# 파라미터 설정
params =      {
                'boosting_type' : 'gbdt',
                'objective'     : 'binary',
                'metric'        : 'auc',
               "learning_rate": 0.01,
               "drop_rate": 0.2,
               "max_depth": 50,
               "num_leaves": 50,
              #  "feature_fraction": 0.5,
              #  "bagging_fraction": 0.5,
              #  "bagging_freq": 100,
                'seed': 1015
                }
#-------------------------------------------------------------------------------------
# 5 Kfold cross validation
k_fold = KFold(n_splits=5, shuffle=True, random_state=421)
for train_idx, val_idx in k_fold.split(train_x):

    # split train, validation set
    X = train_x[train_idx]
    y = train_y[train_idx]
    valid_x = train_x[val_idx]
    valid_y = train_y[val_idx]

    d_train= lgb.Dataset(X, y)
    d_val  = lgb.Dataset(valid_x, valid_y)
    
    #run traning
    model = lgb.train(
                        params,
                        train_set       = d_train,
                        num_boost_round = 500,
                        valid_sets      = d_val,
                        feval           = f_pr_auc,
                        verbose_eval    = 20, 
                        early_stopping_rounds = 100
                       )
    
    # cal valid prediction
    valid_prob = model.predict(valid_x)
    valid_pred = np.where(valid_prob > threshold, 1, 0)
    
    # cal scores
    recall    = recall_score(    valid_y, valid_pred)
    precision = precision_score( valid_y, valid_pred)
    auc_score = roc_auc_score(   valid_y, valid_prob)

    # append scores
    models.append(model)
    recalls.append(recall)
    precisions.append(precision)
    auc_scores.append(auc_score)

    print('==========================================================')

"""# 4. 교차검증 점수 확인"""

print(np.mean(auc_scores))

"""# 5. 제출 파일 생성"""

test_err = pd.read_csv(PATH+'/test_err_data.csv')
display(test_err.head())
display(test_err.info())
display(test_err.describe())

# 데이터 설명을 확인하면
# test 데이터는 ueser_id가 30000부터 44998까지 총 14999개가 존재.
test_user_id_max = 44998
test_user_id_min = 30000
test_user_number = 14999

id_error = test_err[['user_id','errtype']].values
test_x = np.zeros((test_user_number,42))
for person_idx, err in tqdm(id_error):
    # person_idx - test_user_id_min 위치에 person_idx, errtype에 해당하는 error값을 +1
    test_x[person_idx - test_user_id_min,err - 1] += 1
test_x = test_x.reshape(test_x.shape[0],-1)
print(test_x.shape)

# The user_id == 43262 was missed in test_err

MissingUserId = 43262-test_user_id_min
test_err.head()
test_err2 = pd.DataFrame({'user_id': [MissingUserId], 'time': [0], 'model_nm':['NaN'], "fwver":['00.00.0000'], "errtype":[0], "errcode":[0]  })
test_err2 = test_err2.append(test_err)
test_err2 = test_err2.sort_values("user_id", ascending=True)

test_err["user_id"].value_counts().sort_values().unique().shape

"""#### New Variable """

test_err2.info()
test_err2["errcode_numeric"] = 0
test_err2["errcode_numeric"] = test_err2["errcode"].map( lambda x: 1 if str(x).replace("-","").isdigit() else 0 )

test_err2["errcode_digits"] = 0
test_err2["errcode_digits"] = test_err2["errcode"].map(lambda s: len(re.findall( "[0-9]", str(s) )) if str(s).isdigit() else 0)

test_err2["fwver_1"] = 0
test_err2["fwver_1"] = test_err2["fwver"].str.split(".").map(lambda x: x[0]).astype(str).astype(int)
test_err2["fwver_2"] = 0
test_err2["fwver_2"] = test_err2["fwver"].str.split(".").map(lambda x: x[1]).astype(str).astype(int)
test_err2["fwver_3"] = 0
test_err2["fwver_3"] = test_err2["fwver"].str.split(".").map(lambda x: x[2]).astype(str).astype(int)

# test_err2.drop(["fwver"], axis=1, inplace=True)



test_err_NewVariable = test_err2.groupby(["user_id"], as_index=True).agg([npmean, 'count', "min", "max", npstd])

test_err_NewVariable.shape

test_x2 = np.concatenate([test_x, np.array(test_err_NewVariable)], axis=1)

# 예측
pred_y_list = []
for model in models:
    pred_y = model.predict(test_x2)
    pred_y_list.append(pred_y.reshape(-1,1))
    
pred_ensemble = np.mean(pred_y_list, axis = 0)

pred_ensemble

sample_submission = pd.read_csv(PATH+"/sample_submission.csv")

sample_submission['problem'] = pred_ensemble.reshape(-1)

sample_submission.to_csv("dacon_baseline.csv", index = False)
sample_submission

